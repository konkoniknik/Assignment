{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation: \n",
    "\n",
    "We compare the full sets of predicted and reference phrases, labeling items as 1 (present) or 0 (not present) for each set, and then computing standard classification metrics like accuracy, precision, recall, and F1-score, as well as edit distance and BLEU score. We finally save per-row details for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 69.11%\n",
      "Precision: 84.37%\n",
      "Recall: 79.25%\n",
      "F1 Score: 81.73%\n",
      "Average Edit Distance: 5.16\n",
      "Average BLEU Score: 0.70\n",
      "Detailed results with edit distance and BLEU score have been saved to data/output_with_details.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.metrics import edit_distance\n",
    "\n",
    "\n",
    "# Function to split CLEAN_TEXT by '/' based on labeling convention\n",
    "def split_text(text):\n",
    "    if not text or not isinstance(text, str):\n",
    "        return []  # Handle empty or invalid inputs\n",
    "    return [part.strip() for part in text.split('/') if part.strip()]\n",
    "\n",
    "# Function to calculate BLEU score with special handling for single-token cases\n",
    "def calculate_bleu(reference, predicted):\n",
    "    smoothing_fn = SmoothingFunction().method1\n",
    "\n",
    "    # If only one token, use a simple string comparison (only 2-gram+ in the implementation from nltk)\n",
    "    if len(reference) == 1 and len(predicted) == 1:\n",
    "        return 1.0 if reference[0] == predicted[0] else 0.0\n",
    "\n",
    "    # Otherwise, compute BLEU score normally\n",
    "    return sentence_bleu([reference], predicted, weights=(0.5, 0.5), smoothing_function=smoothing_fn)\n",
    "\n",
    "\n",
    "# Function to calculate metrics per row\n",
    "def calculate_metrics(row):\n",
    "    # Extract the reference and predicted sets\n",
    "    reference = set(split_text(row['CLEAN_TEXT']))\n",
    "      # Convert to list\n",
    "    predicted = set(split_text(row['NER_OUT_drop_(0.25)']))    \n",
    "    # Combine all unique phrases\n",
    "    all_phrases = list(reference | predicted)\n",
    "    \n",
    "    # Create binary labels for reference and predicted\n",
    "    reference_labels = [1 if phrase in reference else 0 for phrase in all_phrases]\n",
    "    predicted_labels = [1 if phrase in predicted else 0 for phrase in all_phrases]\n",
    "    \n",
    "    # Compute edit distance and BLEU score\n",
    "    edit_dist = edit_distance(' '.join(reference), ' '.join(predicted))\n",
    "    bleu = calculate_bleu(list(reference), list(predicted))\n",
    "    #print(reference,predicted, edit_dist, bleu)\n",
    "    # Return the metrics\n",
    "    return reference_labels, predicted_labels, all_phrases, edit_dist, bleu\n",
    "\n",
    "\n",
    "# Load the processed CSV\n",
    "file_path = \"data/processed_data_test.csv\"  \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Apply metrics calculation to each row\n",
    "metrics = df.apply(calculate_metrics, axis=1)\n",
    "\n",
    "# Flatten the results for global metrics computation\n",
    "reference_labels = []\n",
    "predicted_labels = []\n",
    "all_phrases = []\n",
    "edit_distances = []\n",
    "bleu_scores = []\n",
    "\n",
    "for ref, pred, phrases, edit_dist, bleu in metrics:\n",
    "    reference_labels.extend(ref)\n",
    "    predicted_labels.extend(pred)\n",
    "    all_phrases.extend(phrases)\n",
    "    edit_distances.append(edit_dist)\n",
    "    bleu_scores.append(bleu)\n",
    "\n",
    "# Calculate overall metrics\n",
    "precision = precision_score(reference_labels, predicted_labels, zero_division=0)\n",
    "recall = recall_score(reference_labels, predicted_labels, zero_division=0)\n",
    "f1 = f1_score(reference_labels, predicted_labels, zero_division=0)\n",
    "\n",
    "# Logging also the accuracy and CM for completion, though the accuracy should  not be very indicative in this task\n",
    "accuracy = accuracy_score(reference_labels, predicted_labels)\n",
    "cm = confusion_matrix(reference_labels, predicted_labels) # Expected 0 predicted and true negatives\n",
    "\n",
    "# Calculate overall average metrics\n",
    "avg_edit_distance = sum(edit_distances) / len(edit_distances)\n",
    "avg_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
    "\n",
    "# Print overall metrics\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Precision: {precision * 100:.2f}%\")\n",
    "print(f\"Recall: {recall * 100:.2f}%\")\n",
    "print(f\"F1 Score: {f1 * 100:.2f}%\")\n",
    "print(f\"Average Edit Distance: {avg_edit_distance:.2f}\")\n",
    "print(f\"Average BLEU Score: {avg_bleu_score:.2f}\")\n",
    "\n",
    "\n",
    "df['reference_labels'] = [ref for ref, _, _, _, _ in metrics]\n",
    "df['predicted_labels'] = [pred for _, pred, _, _, _ in metrics]\n",
    "df['all_phrases'] = [phrases for _, _, phrases, _, _ in metrics]\n",
    "df['edit_distance'] = edit_distances\n",
    "df['bleu_score'] = \n",
    "\n",
    "# Save detailed results for debugging (optional)\n",
    "output_with_details = \"data/output_with_details.csv\"\n",
    "df.to_csv(output_with_details, index=False)\n",
    "print(f\"Detailed results with edit distance and BLEU score have been saved to {output_with_details}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Rule based** (simple preproc):\n",
    "Accuracy: 71.79%\n",
    "Precision: 80.42%\n",
    "Recall: 86.99%\n",
    "F1 Score: 83.58%\n",
    "Average Edit Distance: 4.28\n",
    "Average BLEU Score: 0.78\n",
    "\n",
    "**NER only** r=0.25:\n",
    "Accuracy: 69.11%\n",
    "Precision: 84.37%\n",
    "Recall: 79.25%\n",
    "F1 Score: 81.73%\n",
    "Average Edit Distance: 5.16\n",
    "Average BLEU Score: 0.70\n",
    "\n",
    "\n",
    "**NER & POS** r=(0.25,0.25):\n",
    "Accuracy: 71.99%\n",
    "Precision: 80.76%\n",
    "Recall: 86.90%\n",
    "F1 Score: 83.72%\n",
    "Average Edit Distance: 4.25\n",
    "Average BLEU Score: 0.78\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**LLM** (llama-3.1-8B) (Small sample, **not full result!**):\n",
    "Accuracy: 79.31%\n",
    "Precision: 88.46%\n",
    "Recall: 88.46%\n",
    "F1 Score: 88.46%\n",
    "Average Edit Distance: 2.40\n",
    "Average BLEU Score: 0.73\n",
    "\n",
    "**DL** (has not converged appropriately. Maybe more training?)\n",
    "Accuracy: 1.85%\n",
    "Precision: 2.13%\n",
    "Recall: 12.35%\n",
    "F1 Score: 3.63%\n",
    "Average Edit Distance: 61.23\n",
    "Average BLEU Score: 0.01\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
