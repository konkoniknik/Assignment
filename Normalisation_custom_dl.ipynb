{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom DL Model (PyTorch)\n",
    "\n",
    "We also train a  Transformer encoder model to automatically perform normalisation. The model takes as input an raw string and  trains on the respective  CLEAN TEXT string as the target. We use character level tokenization, and the torch framework to train a small 4 layer encoder transformer on this task. We use cross entropy loss on an unembed (embed to vocab) layer to predict the appropriate  letters of the output based on the clean text. \n",
    "\n",
    "There is a lot room for improvement as we dont have a lot of data, or a lot of compute either to train or to evaluate. Apart from more data  and compute, we suspect that a better mapping between the input and output data would be beneficial (where potentially each letter maps if possible directly in input and output poisitions). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\konni\\venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "c:\\Users\\konni\\venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "843 [' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', '«', '»', 'Á', 'Â', 'Ä', 'Å', 'É', 'Í', 'Ó', 'Ö', 'Ø', 'ß', 'à', 'á', 'â', 'ã', 'ä', 'å', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'ï', 'ñ', 'ó', 'ô', 'õ', 'ö', 'ø', 'ù', 'ú', 'ü', 'ý', 'ă', 'ć', 'Č', 'č', 'Đ', 'ě', 'Ģ', 'ĩ', 'ī', 'ı', 'Ł', 'ł', 'ń', 'ņ', 'ř', 'Ş', 'ş', 'Š', 'š', 'ũ', 'ū', 'Ż', 'Ə', 'ơ', 'ư', 'ə', '́', 'Α', 'Γ', 'Δ', 'Ε', 'Η', 'Θ', 'Ι', 'Κ', 'Λ', 'Μ', 'Ξ', 'Ο', 'Ρ', 'Σ', 'Τ', 'Υ', 'Ω', 'ά', 'α', 'κ', 'ο', 'ρ', 'ς', 'τ', 'χ', 'ύ', 'І', 'А', 'Б', 'В', 'Г', 'Д', 'Е', 'З', 'И', 'К', 'Л', 'М', 'Н', 'О', 'П', 'Р', 'С', 'Т', 'Х', 'Ц', 'Ч', 'Ш', 'Ю', 'Я', 'а', 'б', 'в', 'г', 'д', 'е', 'ж', 'з', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ы', 'ь', 'э', 'ю', 'я', 'ё', 'і', 'Қ', 'ב', 'ג', 'ד', 'ה', 'ו', 'ח', 'ט', 'י', 'ך', 'ל', 'ם', 'מ', 'ן', 'נ', 'ס', 'ף', 'ר', 'ש', 'ת', 'آ', 'أ', 'ا', 'ب', 'ة', 'ت', 'ج', 'ح', 'خ', 'د', 'ر', 'ز', 'س', 'ش', 'ص', 'ض', 'ط', 'ع', 'ف', 'ق', 'ك', 'ل', 'م', 'ن', 'ه', 'و', 'ى', 'ي', 'پ', 'ی', 'ก', 'ง', 'จ', 'ช', 'ฐ', 'ณ', 'ด', 'ต', 'น', 'บ', 'ป', 'พ', 'ย', 'ร', 'ล', 'ศ', 'ษ', 'ส', 'ห', 'ฬ', 'อ', 'ะ', 'ั', 'ำ', 'ิ', 'ี', 'ุ', 'เ', 'แ', 'โ', '์', 'ក', 'ខ', 'គ', 'ឃ', 'ង', 'ដ', 'ឋ', 'ត', 'ទ', 'ន', 'ផ', 'ព', 'ភ', 'ម', 'យ', 'រ', 'ល', 'ស', 'ហ', 'ា', 'ិ', 'ី', 'ុ', 'ូ', 'ួ', 'េ', 'ំ', 'ៈ', '៉', '៊', '់', '្', 'ạ', 'ả', 'ấ', 'ầ', 'ậ', 'ắ', 'ằ', 'ặ', 'ế', 'ề', 'ể', 'ễ', 'ệ', 'ị', 'ọ', 'ố', 'ỗ', 'ộ', 'ớ', 'ờ', 'ụ', 'ủ', 'ứ', 'ừ', 'ử', 'ữ', 'ự', 'ỳ', 'ỹ', '\\u200b', '‐', '–', '’', '“', '”', '′', '″', '☆', '♰', '\\u3000', '〜', 'あ', 'い', 'う', 'か', 'き', 'ぎ', 'し', 'じ', 'た', 'ち', 'て', 'ど', 'な', 'に', 'は', 'ひ', 'ふ', 'ま', 'み', 'ゃ', 'や', 'ゆ', 'よ', 'ら', 'り', 'ろ', 'を', 'ん', 'ア', 'ィ', 'イ', 'オ', 'カ', 'キ', 'グ', 'ケ', 'コ', 'ゴ', 'ス', 'ソ', 'タ', 'ッ', 'ツ', 'テ', 'ト', 'ナ', 'ネ', 'ノ', 'ビ', 'ピ', 'ブ', 'マ', 'ャ', 'ヤ', 'ラ', 'リ', 'ル', 'レ', 'ン', '・', 'ー', 'ㅤ', '一', '七', '三', '上', '下', '世', '业', '中', '久', '之', '乐', '也', '云', '五', '井', '亜', '京', '仁', '今', '介', '代', '伊', '传', '伴', '伶', '伸', '佐', '余', '作', '佳', '信', '倉', '倫', '偉', '傑', '元', '光', '六', '兰', '兴', '冉', '冠', '准', '出', '刘', '利', '則', '前', '动', '勅', '勝', '北', '升', '南', '博', '原', '又', '及', '口', '只', '号', '司', '名', '周', '哲', '啟', '喝', '嘉', '國', '土', '垛', '増', '士', '大', '天', '太', '夫', '奥', '奴', '姜', '婷', '子', '孫', '宏', '宗', '実', '宮', '富', '寛', '寿', '小', '屋', '屠', '山', '岩', '島', '川', '巣', '巫', '市', '平', '幸', '広', '康', '廣', '弓', '弘', '张', '弦', '強', '弾', '彼', '徐', '得', '徳', '徹', '志', '悟', '悠', '悦', '想', '愛', '慶', '憲', '應', '我', '摘', '敏', '敦', '敬', '文', '斐', '料', '方', '日', '旻', '昇', '明', '星', '昭', '晴', '暁', '月', '朗', '木', '末', '本', '朴', '杉', '李', '村', '条', '来', '杰', '松', '林', '枝', '柊', '柳', '栗', '桑', '梁', '條', '森', '植', '椿', '楊', '榮', '樫', '樹', '橋', '櫻', '權', '欽', '歌', '水', '江', '沅', '沙', '沢', '河', '治', '泓', '泳', '浦', '浩', '海', '涛', '涵', '淳', '清', '渡', '源', '潘', '潤', '潮', '澤', '瀬', '煙', '爽', '王', '玲', '瑋', '瑞', '瓊', '甘', '生', '田', '申', '百', '盛', '直', '眞', '真', '眠', '知', '石', '祐', '神', '福', '穂', '穎', '穴', '立', '竹', '米', '純', '細', '織', '统', '美', '義', '臣', '花', '芳', '芹', '英', '茂', '范', '茶', '草', '莊', '莎', '菅', '菊', '菜', '華', '蒼', '蔣', '藤', '虹', '融', '蟲', '行', '補', '見', '角', '許', '詞', '詩', '語', '譲', '许', '谷', '豪', '貴', '賀', '賢', '越', '輪', '辛', '辺', '近', '远', '進', '郎', '部', '鄭', '醒', '重', '野', '鐘', '間', '関', '阪', '阿', '陈', '陳', '陶', '陽', '隆', '隈', '雅', '雨', '青', '韦', '韬', '音', '韻', '風', '飲', '驄', '高', '鳴', '鷺', '麻', '黃', '黎', '龍', '갑', '강', '개', '경', '교', '국', '균', '기', '길', '김', '남', '낭', '노', '니', '더', '도', '동', '드', '디', '라', '랭', '량', '럼', '리', '민', '바', '박', '밤', '범', '베', '사', '상', '서', '소', '손', '송', '숀', '수', '스', '승', '시', '신', '심', '써', '암', '영', '오', '옥', '용', '우', '원', '위', '유', '은', '음', '의', '이', '자', '재', '정', '주', '준', '중', '지', '철', '첼', '최', '코', '쿼', '트', '핀', '혁', '현', '혜', '호', '홍', '화', '훈', '희', '（', '）', '／', '：']\n",
      "Device: cuda\n",
      "Epoch 1/10, Train Loss: 1.4302, Val Loss: 1.1059\n",
      "Epoch 2/10, Train Loss: 1.0131, Val Loss: 1.0506\n",
      "Epoch 3/10, Train Loss: 0.9820, Val Loss: 1.0358\n",
      "Epoch 4/10, Train Loss: 0.9545, Val Loss: 1.0461\n",
      "Epoch 5/10, Train Loss: 0.9569, Val Loss: 1.0244\n",
      "Epoch 6/10, Train Loss: 0.9507, Val Loss: 1.0272\n",
      "Epoch 7/10, Train Loss: 0.9414, Val Loss: 1.0269\n",
      "Epoch 8/10, Train Loss: 0.9367, Val Loss: 1.0305\n",
      "Epoch 9/10, Train Loss: 0.9378, Val Loss: 1.0308\n",
      "Epoch 10/10, Train Loss: 0.9304, Val Loss: 1.0295\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(\"data/raw/normalization_assesment_dataset_10k.csv\")  \n",
    "\n",
    "# Split into train, val, test\n",
    "train_df, val_df, test_df = np.split(df.sample(frac=1, random_state=42), \n",
    "                                    [int(0.5 * len(df)), int(0.75 * len(df))])\n",
    "\n",
    "\n",
    "train_df[\"raw_comp_writers_text\"] = train_df[\"raw_comp_writers_text\"].fillna(\"\").astype(str)\n",
    "train_df[\"CLEAN_TEXT\"] = train_df[\"CLEAN_TEXT\"].fillna(\"\").astype(str)\n",
    "\n",
    "val_df[\"raw_comp_writers_text\"] = val_df[\"raw_comp_writers_text\"].fillna(\"\").astype(str)\n",
    "val_df[\"CLEAN_TEXT\"] = val_df[\"CLEAN_TEXT\"].fillna(\"\").astype(str)\n",
    "\n",
    "test_df[\"raw_comp_writers_text\"] = test_df[\"raw_comp_writers_text\"].fillna(\"\").astype(str)\n",
    "test_df[\"CLEAN_TEXT\"] = test_df[\"CLEAN_TEXT\"].fillna(\"\").astype(str)\n",
    "\n",
    "# Character-level vocabulary\n",
    "chars = sorted(set(\"\".join(train_df[\"raw_comp_writers_text\"]) + \"\".join(train_df[\"CLEAN_TEXT\"])))\n",
    "char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
    "idx_to_char = {idx: ch for ch, idx in char_to_idx.items()}\n",
    "print(len(chars), chars)\n",
    "\n",
    "# Tokenization\n",
    "def tokenize(text, char_to_idx, max_len=100):\n",
    "    tokenized = [char_to_idx[ch] for ch in text if ch in char_to_idx]\n",
    "    return tokenized[:max_len] + [0] * (max_len - len(tokenized))\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df, char_to_idx, max_len=100):\n",
    "        self.raw_texts = df[\"raw_comp_writers_text\"].tolist()\n",
    "        self.clean_texts = df[\"CLEAN_TEXT\"].tolist()\n",
    "        self.char_to_idx = char_to_idx\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        raw = tokenize(self.raw_texts[idx], self.char_to_idx, self.max_len)\n",
    "        clean = tokenize(self.clean_texts[idx], self.char_to_idx, self.max_len)\n",
    "        return torch.tensor(raw, dtype=torch.long), torch.tensor(clean, dtype=torch.long)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "max_len = 100\n",
    "batch_size=32\n",
    "train_dataset = TextDataset(train_df, char_to_idx, max_len)\n",
    "val_dataset = TextDataset(val_df, char_to_idx, max_len)\n",
    "test_dataset = TextDataset(test_df, char_to_idx, max_len)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Transformer Encoder Model\n",
    "class TransformerEncoderModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, ff_dim, num_layers, max_len):\n",
    "        super(TransformerEncoderModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, max_len, embed_dim))\n",
    "        # Default not causal, as we want in this case\n",
    "        encoder_layer = nn.TransformerEncoderLayer(embed_dim, num_heads, ff_dim) \n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embed = self.embedding(src) + self.positional_encoding[:, :src.size(1), :]\n",
    "        encoded = self.transformer_encoder(embed)\n",
    "        output = self.fc(encoded)\n",
    "        return output\n",
    "\n",
    "# Model configuration\n",
    "vocab_size = len(chars)\n",
    "embed_dim = 128\n",
    "num_heads = 8\n",
    "ff_dim = 512\n",
    "num_layers = 4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\",\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TransformerEncoderModel(vocab_size, embed_dim, num_heads, ff_dim, num_layers, max_len).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for raw, clean in dataloader:\n",
    "        raw, clean = raw.to(device), clean.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(raw)\n",
    "        loss = criterion(output.view(-1, vocab_size), clean.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Validation loop\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for raw, clean in dataloader:\n",
    "            raw, clean = raw.to(device), clean.to(device)\n",
    "            output = model(raw)\n",
    "            loss = criterion(output.view(-1, vocab_size), clean.view(-1))\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Training the model\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss = evaluate(model, val_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Save the model?\n",
    "#torch.save(model.state_dict(), \"models/transformer_encoder_model.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform inference on validation dataset, and show predictions and targets. Notice that the output captures key characters of the input but has a lot of noise in irrelevant positions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Input                                          | Predicted Output                                   | Ground Truth                                      \n",
      "======================================================================================================================================================\n",
      "Ihor Vitsinskyy (BMI IPI#767439101)                                                                  | Ihor/Vitsinskyya(BMIaIPI#767439101)aaaaaaaa/aaa//aaaaaaaa/aaaaaaaaaa//aaaaaa/aa/a/aa/aaaa//a/aaaa/aa | Ihor Vitsinskyy                                                                                     \n",
      "Daniel Kim (김니)                                                                                      | Daniel/Kim/(Pu)aa/aaaaaaaaaaaaaaaa/aaaaaaaa/aaa//aaaaaaaa/aaaaaaaaaa//aaaaaa/aa/a/aa/aaaa//a/aaaa/aa | Daniel Kim                                                                                          \n",
      "Tatsuji Kimura/Miyako Koda                                                                           | Tatsuji/Kimura/MiyakoaKodaaaaaaaaa/aaaaaaaa/aaa//aaaaaaaa/aaaaaaaaaa//aaaaaa/aa/a/aa/aaaa//a/aaaa/aa | Tatsuji Kimura/Miyako Koda                                                                          \n",
      "Ness Beats                                                                                           | Ness/Beats/////aa/aaaaaaaaaaaaaaaa/aaaaaaaa/aaa//aaaaaaaa/aaaaaaaaaa//aaaaaa/aa/a/aa/aaaa//a/aaaa/aa |                                                                                                     \n",
      "C.Bringtown                                                                                          | C.Bringtown////aa/aaaaaaaaaaaaaaaa/aaaaaaaa/aaa//aaaaaaaa/aaaaaaaaaa//aaaaaa/aa/a/aa/aaaa//a/aaaa/aa | C.Bringtown                                                                                         \n",
      "Halim Nassif                                                                                         | Halim/Nassif///aa/aaaaaaaaaaaaaaaa/aaaaaaaa/aaa//aaaaaaaa/aaaaaaaaaa//aaaaaa/aa/a/aa/aaaa//a/aaaa/aa |                                                                                                     \n",
      "Magnus Brochs                                                                                        | Magnus/Brochs//aa/aaaaaaaaaaaaaaaa/aaaaaaaa/aaa//aaaaaaaa/aaaaaaaaaa//aaaaaa/aa/a/aa/aaaa//a/aaaa/aa | Magnus Brochs                                                                                       \n",
      "Mark Fox/Thom Blunier/Thomas Muster                                                                  | Mark/Fox/Thom/Blunier/ThomasaMusteraaaaaaaa/aaa//aaaaaaaa/aaaaaaaaaa//aaaaaa/aa/a/aa/aaaa//a/aaaa/aa | Mark Fox/Thom Blunier/Thomas Muster                                                                 \n",
      "Tex Lecor/Jacques Labrecque                                                                          | Tex/Lecor/Jacques/Labrecqueaaaaaaa/aaaaaaaa/aaa//aaaaaaaa/aaaaaaaaaa//aaaaaa/aa/a/aa/aaaa//a/aaaa/aa |                                                                                                     \n",
      "Sean Phillip Penner & Collin Deatherage                                                              | Sean/Phillip/PenneraaaCollinaDeatherageaaaa/aaa//aaaaaaaa/aaaaaaaaaa//aaaaaa/aa/a/aa/aaaa//a/aaaa/aa | Sean Phillip Penner/Collin Deatherage                                                               \n"
     ]
    }
   ],
   "source": [
    "# Validation inference function\n",
    "def infer_on_validation_set(model, dataloader, idx_to_char, device, num_samples=10):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for raw, clean in dataloader:\n",
    "            raw = raw.to(device)\n",
    "            output = model(raw)\n",
    "            # Get the most likely token at each position (move tensor back to cpu to perform numpy operations)\n",
    "            pred = torch.argmax(output, dim=-1).cpu().numpy()\n",
    "            raw = raw.cpu().numpy()\n",
    "            clean = clean.cpu().numpy()\n",
    "            # Decode and append\n",
    "            for i in range(min(len(raw), num_samples)):\n",
    "                raw_text = \"\".join(idx_to_char[idx] for idx in raw[i] if idx in idx_to_char)\n",
    "                pred_text = \"\".join(idx_to_char[idx] for idx in pred[i] if idx in idx_to_char)\n",
    "                clean_text = \"\".join(idx_to_char[idx] for idx in clean[i] if idx in idx_to_char)\n",
    "                predictions.append((raw_text, pred_text, clean_text))\n",
    "            if len(predictions) >= num_samples:\n",
    "                break\n",
    "    return predictions[:num_samples]\n",
    "\n",
    "# Run inference on the validation set\n",
    "num_samples_to_inspect = 10\n",
    "validation_predictions = infer_on_validation_set(model, val_loader, idx_to_char, device, num_samples_to_inspect)\n",
    "\n",
    "# Display the predictions\n",
    "print(f\"{'Raw Input':<50} | {'Predicted Output':<50} | {'Ground Truth':<50}\")\n",
    "print(\"=\" * 150)\n",
    "for raw_text, pred_text, clean_text in validation_predictions:\n",
    "    print(f\"{raw_text:<50} | {pred_text:<50} | {clean_text:<50}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on the validation set\n",
    "num_samples_to_inspect = 2500\n",
    "test_predictions = infer_on_validation_set(model, test_loader, idx_to_char, device, num_samples=num_samples_to_inspect)\n",
    "\n",
    "raw_list,clean_list,preds_list = [],[],[]\n",
    "for raw_text, pred_text, clean_text in test_predictions:\n",
    "    raw_list.append(raw_text)\n",
    "    clean_list.append(clean_text)\n",
    "    preds_list.append(pred_text)\n",
    "\n",
    "dl_df =  pd.DataFrame({\n",
    "    \"RAW_TEXT\": raw_list,\n",
    "    \"CLEAN_TEXT\": clean_list,\n",
    "    \"DL_OUT\": preds_list\n",
    "})\n",
    "\n",
    "\n",
    "output_file_path = \"output_file_dl.csv\"  \n",
    "dl_df.to_csv(output_file_path, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
